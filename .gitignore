llama.cpp/quantize/gguf_models
llama.cpp/quantize/original_models
mozilla-ocho/llava-*.llamafile

# Ignore log directories where I capture output.
dclm/logs/
llama.cpp/quantize/logs/
mojo/logs/
mozilla-ocho/logs/
ollama/play/logs/
